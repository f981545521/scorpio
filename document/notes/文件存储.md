
## 文件系统





FastDFS 主要用于大中网站，为文件上传和下载提供在线服务。所以在负载均衡、动态扩容等方面都支持得比较好，FastDFS不会对文件进行分快（切分）存储。

HDFS 主要解决并行计算中分布式存储数据的问题。其单个数据文件通常很大，采用了分块（切分）存储的方式；

MongoDBGridFS MongoDB的文件存储系统


#### HBase
HDFS是Hadoop分布式文件系统。
HBase的数据通常存储在HDFS上。HDFS为HBase提供了高可靠性的底层存储支持。
Hbase是Hadoop database即Hadoop数据库。它是一个适合于非结构化数据存储的数据库，HBase基于列的而不是基于行的模式。
HBase是Google Bigtable的开源实现，类似Google Bigtable利用GFS作为其文件存储系统，HBase利用Hadoop HDFS作为其文件存储系统；Google运行MapReduce来处理Bigtable中的海量数据，HBase同样利用Hadoop MapReduce来处理HBase中的海量数据。
HDFS为HBase提供了高可靠性的底层存储支持，Hadoop MapReduce为HBase提供了高性能的计算能力，Zookeeper为HBase提供了稳定服务和failover机制。Pig和Hive还为HBase提供了高层语言支持，使得在HBase上进行数据统计处理变的非常简单。 Sqoop则为HBase提供了方便的RDBMS（关系型数据库）数据导入功能，使得传统数据库数据向HBase中迁移变的非常方便。


HBase 本身其实可以完全不要考虑 HDFS 的，你完全可以只把 HBase 当作是一个分布式高并发 k-v 存储系统，只不过它底层的文件系统是通过 HDFS 来支持的罢了。换做其他的分布式文件系统也是一样的，不影响 HBase 的本质。甚至如果你不考虑文件系统的分布式或稳定性等特性的话，完全可以用简单的本地文件系统，甚至内存文件系统来代替。

HBase 在 HDFS 之上提供了：

①、高并发实时随机写，通过 LSM（内存+顺序写磁盘）的方式提供了 HDFS 所不拥有的实时随机写及修改功能

②、高并发实时点读及扫描了解一下 LSM 算法，在文件系统之上有数据库，在业务层面，HBase 完全可以独立于 HDFS 来理解



## HDFS 与 Hadoop 紧密相连

安装Hadoop参考		:https://www.cnblogs.com/zhi-leaf/p/11496877.html

https://hadoop.apache.org/releases.html
https://mirror.bit.edu.cn/apache/hadoop/common/hadoop-3.1.3/hadoop-3.1.3-src.tar.gz
https://mirror.bit.edu.cn/apache/hadoop/common/hadoop-3.1.3/hadoop-3.1.3.tar.gz

export HADOOP_HOME=/home/hadoop/hadoop-3.1.3
export PATH=$HADOOP_HOME/bin:$PATH

export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.252.b09-2.el7_8.x86_64
export JRE_HOME=${JAVA_HOME}/jre
export CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib
export PATH=${JAVA_HOME}/bin:$PATH

1. 安装JDK、下载Hadoop
2. 配置JAVA_HOME
```
	vi etc/hadoop/hadoop-env.sh
	// 修改为
	export JAVA_HOME=/usr/local/jdk1.8.0_221/
```
3. vi etc/hadoop/core-site.xml
```
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://0.0.0.0:9000</value>
    </property>
</configuration>
```
4. vi etc/hadoop/hdfs-site.xml
```
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
</configuration>
```
5. 设置主机允许无密码SSH链接
```
ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa       // 创建公钥私钥对
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys //
chmod 0600 ~/.ssh/authorized_keys // 设置权限，owner有读写权限，group和other无权限
```
6. 格式化文件系统
bin/hdfs namenode -format
7. 启动NameNode和DataNode进程（启动hdfs）
./sbin/start-dfs.sh // 启动NameNode和DataNode进程
./sbin/stop-dfs.sh  // 关闭NameNode和DataNode进程
8. 启动YARN
./sbin/start-yarn.sh
./sbin/stop-yarn.sh




### WEB管理

http://192.168.1.102:50070


http://192.168.1.102:8088/cluster

### 解决：hadoop无法访问50070端口
vi etc/hadoop/hdfs-site.xml
<property>
  <name>dfs.http.address</name>
  <value>0.0.0.0:50070</value>
</property>









